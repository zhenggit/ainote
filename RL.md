# RL notebook

From SFT, PPO to GRPO and some following RL algorithms.

## SFT

### Objective

```math
J_{SFT}(\theta) = \mathbb{E}_{x, y \sim P_{\mathrm{sft}} (X, Y)} \left[ \frac{1}{|y|} \sum_{t=1}^{|y|} \log \pi_\theta (y_t | x, y_{< t}) \right],
```
where $`x`$ is the prompt and $`y = (y_1, y_2, \cdots)`$ is the answer. $`P_{\mathrm{sft}} (X, Y)`$ is the distribution of SFT trainning data. This is the cross entropy for the next token prediction.

### Gradient

```math
\nabla_{\theta} J_{SFT}(\theta) = \mathbb{E}_{x, y \sim P_{\mathrm{sft}} (X, Y)} \left[ \frac{1}{|y|} \sum_{t=1}^{|y|} \nabla_\theta \log \pi_\theta (y_t | x, y_{< t}) \right]
```

## PPO

### Objective

```math
J_{\mathrm{PPO}} (\theta) = \mathbb{E}_{x \sim P(X), y \sim \pi_{\mathrm{old}} (Y | x)} \left[
  \frac{1}{|y|} \sum_{t=1}^{|y|} S_\epsilon \left( \frac{\pi_\theta (y_t | x, y_{< t})}{\pi_{\mathrm{old}} (y_t | x, y_{< t})}, A_t \right)
\right]
```
where  $`S_\epsilon(r, a) = \min \{ra, \mathrm{clip}(r, 1-\epsilon, 1+\epsilon) a \}`$.

The advantage (from GAE alg.) $`A_t`$ is defined as

```math
A_t = \delta_t + \gamma \lambda \delta_{t+1} + \cdots + (\gamma \lambda)^{T-t+1} \delta_{T-1},
```
with $`\delta_t = r_t + \gamma V_\phi(s_{t+1}) - V_\phi (s_t)`$ and $`V_\phi (s)`$ is the value function trained alongside the policy model. 

Here, $`r_t`$ is the per-token reward model

```math
r_t = r (x, y_{\le t})- \beta \log \frac{\pi_\theta (y_t | x, y_{< t})}{\pi_\mathrm{ref} (y_t | x, y_{< t})}
```
where $`r(x,y_{\le t})`$ is a (raw) reward model.


### Gradient

Assume $`\pi_\theta = \pi_\mathrm{old}`$.
```math
\nabla_\theta J_{\mathrm{PPO}} (\theta) = \mathbb{E}_{x \sim P(X), y \sim \pi_{\mathrm{old}} (Y | x)} \left[
  \frac{1}{|y|} \sum_{t=1}^{|y|} 
    A_t \nabla_\theta \log \pi_\theta (y_t | x, y_{< t})
\right].
```
$`\color{red}\text{THIS IS FROM GRPO PAPER, WHY???}`$

## DPO

DPO is an off-policy alg (all the training data is generated by $`\pi_{\mathrm{sft}} (Y | x)`$)

### Objective

```math
J_{\mathrm{DPO}} (\theta) = \mathbb{E}_{x \sim P(X), y^+,y^- \sim \pi_{\mathrm{sft}} (Y | x)} 
   \log \sigma \left(
      \beta \cdot \frac{1}{|y^+|} \sum_{t=1}^{|y^+|} \log \frac{\pi_\theta (y^+_t | x, y^+_{< t})}{\pi_{\mathrm{ref}} (y^+_t | x, y^+_{< t})} -
      \beta \cdot \frac{1}{|y^-|} \sum_{t=1}^{|y^-|} \log \frac{\pi_\theta (y^-_t | x, y^-_{< t})}{\pi_{\mathrm{ref}} (y^-_t | x, y^-_{< t})}
  \right)
```
where 

```math
\sigma(z) = \frac{1}{1+\exp(-z)} \quad \mathrm{with} \quad \frac{\partial log \sigma(z)}{\partial z} = \sigma(-z)
```

### Gradient

```math
\nabla_\theta J_{\mathrm{DPO}} (\theta) = \mathbb{E}_{x \sim P(X), y^+,y^- \sim \pi_{\mathrm{sft}} (Y | x)}
\left[
  \beta \frac{1}{|y^+|} \sum_{t=1}^{|y^+|} \varsigma_t \nabla_\theta \log \frac{\pi_\theta (y^+_t | x, y^+_{< t})}{\pi_{\mathrm{ref}} (y^+_t | x, y^+_{< t})} -
  \beta \frac{1}{|y^-|} \sum_{t=1}^{|y^-|} \varsigma_t \nabla_\theta \log \frac{\pi_\theta (y^-_t | x, y^-_{< t})}{\pi_{\mathrm{ref}} (y^-_t | x, y^-_{< t})}
\right],
```
where
```math
\varsigma_t = \sigma \left(
  \beta \frac{1}{|y^-|} \sum_{t=1}^{|y^-|} \log \frac{\pi_\theta (y^-_t | x, y^-_{< t})}{\pi_{\mathrm{ref}} (y^-_t | x, y^-_{< t})} -
  \beta \frac{1}{|y^+|} \sum_{t=1}^{|y^+|} \log \frac{\pi_\theta (y^+_t | x, y^+_{< t})}{\pi_{\mathrm{ref}} (y^+_t | x, y^+_{< t})}
\right)
```

## RFT (Rejection sampling Fine Tunning)

### Objective

```math
J_{\mathrm{RFT}} (\theta) = \mathbb{E}_{x \sim P(X), y \sim \pi_{\mathrm{sft}} (Y | x)} \left[
  \mathbb{I}(y) \cdot \frac{1}{|y|} \sum_{t=1}^{|y|}  \log \pi_\theta (y_t | x, y_{< t})
\right]
```

### Gradient

```math
\nabla_\theta J_\mathrm{RFT} (\theta) = \mathbb{E}_{x \sim P(X), y \sim \pi_{\mathrm{sft}} (Y | x)} \left[
  \mathbb{I}(y) \cdot \frac{1}{|y|} \sum_{t=1}^{|y|}  \nabla_\theta \log \pi_\theta (y_t | x, y_{< t})
\right]
```

## GRPO

### Objective

```math
J_\mathrm{GRPO} (\theta) = \mathbb{E}_{x \sim P(X), \{y_i\}_{i=1}^G \sim \pi_\mathrm{old} (Y|x)}
\frac{1}{G} \sum_{i=1}^G \frac{1}{|y_i|} \sum_{t=1}^{|y_i|}
\left[
  S_\epsilon \left(\frac{\pi_\theta (y_{i,t} | x, y_{i, < t})}{\pi_\mathrm{old} (y_{i, t} | x, y_{i, < t})}, A_{i,t} \right)
  - D \left( \frac{\pi_\mathrm{ref} (y_{i,t} | x, y_{i, < t})}{\pi_\theta (y_{i, t} | x, y_{i, < t})} \right)
\right],
```
where $`S_\epsilon(r, a) = \min \{ra, \mathrm{clip}(r, 1-\epsilon, 1+\epsilon) a\}`$ and $`D(r) = r - \log r -1`$.

## DAPO

### Objective

```math
J_\mathrm{DAPO} (\theta) = \mathbb{E}_{x \sim P(X), \{y_i\}_{i=1}^G \sim \pi_\mathrm{old} (Y|x), \color{red}{ | \{y_i | \text{is\_equivalent}(y_i, \mathrm{ans}(x)) \}| \in (0,G)}}
\color{green}{\frac{1}{\sum_{i=1}^G |y_i|} \sum_{i=1}^G \sum_{t=1}^{|y_i|}} \color{red}{ S_{\epsilon_\mathrm{low}, \epsilon_\mathrm{high}} }  \color{black}{\left(\frac{\pi_\theta (y_{i,t} | x, y_{i, < t})}{\pi_\mathrm{old} (y_{i, t} | x, y_{i, < t})}, A_{i,t} \right) }
```

* Clip-Higher (and removing KL Divergence): $`S_{\epsilon_\mathrm{low}, \epsilon_\mathrm{high}} (r, a) = \min \{ ra, \mathrm{clip} (r, 1-\epsilon_\mathrm{low}, 1+\epsilon_\mathrm{high}) a\}`$. A much larger $`\epsilon_\mathrm{high}`$ prevents entropy collapse and boosts exploration.
* Dynamic Sampling: $`| \{y_i | \text{is\_equivalent}(y_i, \mathrm{ans}(x)) \}| \in (0,G)`$ discards prompts whose accuracy is 0 or 1 so every example contributes gradient signal, improving sample efficiency and convergence speed.
* Token-level Policy Gradient loss $`\frac{1}{\sum_{i=1}^G |y_i|} \sum_{i=1}^G \sum_{t=1}^{|y_i|} S_{\epsilon_\mathrm{low}, \epsilon_\mathrm{high}} (\cdot)`$.
* Overlong Reward Shaping: $`A_{i,t}`$ comes from the reward $`r(\mathrm{ans}(x),y) = R_\mathrm{rule} (\mathrm{ans}(x), y) + R_\mathrm{length}(y)`$, where
```math
R_\mathrm{rule} (\mathrm{ans}(x), y) = \left\{
\begin{array}{ll}
1, & \text{is\_equivalent}(\mathrm{ans}(x), y) \\
-1, & \text{otherwise}.
\end{array}\right.
```
```math
R_\mathrm{length} (y) = \left\{
\begin{array}{ll}
0, & |y| \le L_\max - L_\mathrm{cache}, \\
(L_\max - L_\mathrm{cache} - |y|) / L_\mathrm{cache}, & L_\max - L_\mathrm{cache} < |y| \le L_\max, \\
-1, & |y| > L_\max.
\end{array}
\right.
```

## Some discussion

### reward hacking problem

* Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization, 2022.
* Lilian Weng. Reward hacking in reinforcement learning. lilianweng.github.io, Nov 2024.
